<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>机器学习系统 1-9 - 后端硬件编译器 · KLEON</title><meta name="description" content="编译，广义上是将一种表达映射成另一种表达，维持功能不变。深度学习后端硬件的编译，是将DSL描述的计算图映射成后端硬件指令。

传统编译器直接解析文本，通过词法与语法分析，构建AST1。之后在AST上执行一系列的优化，称为优化pipeline。
优化后的AST可以转换为IR（比如LLVM2 IR），由"><meta name="og:description" content="编译，广义上是将一种表达映射成另一种表达，维持功能不变。深度学习后端硬件的编译，是将DSL描述的计算图映射成后端硬件指令。

传统编译器直接解析文本，通过词法与语法分析，构建AST1。之后在AST上执行一系列的优化，称为优化pipeline。
优化后的AST可以转换为IR（比如LLVM2 IR），由"><meta name="twitter:site" content="KLEON"><meta name="twitter:title" content="机器学习系统 1-9 - 后端硬件编译器"><meta name="twitter:card" content="summary"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/github.min.css"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="https://lf9-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.10.2/katex.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 6.1.0"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title"></a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">自说自话</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/categories">Categories</a></li><li><a href="/tags">Tags</a></li><li><a href="/googlebe23cb0bc55fc412.html"></a></li><li><a href="/about/index.html"></a></li><li class="soc"><a href="https://github.com/kleon1024" target="_blank" rel="noopener noreferrer" aria-label="Github"><i class="fa fa-github">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2022&nbsp;<a target="_blank" href="https://blog.kleon.space" rel="noopener noreferrer">Kleon</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>机器学习系统 1-9 - 后端硬件编译器</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2021-01-26</span><span class="date meta-item">Updated at&nbsp;2021-01-26</span><span class="meta-item"><i class="fa fa-folder"></i><span>&nbsp;</span><a href="/categories/机器学习/" title="机器学习" class="a-tag">机器学习</a><span>&nbsp;</span></span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/机器学习/" title="机器学习" class="a-tag">机器学习</a><span>&nbsp;</span><a href="/tags/FPGA/" title="FPGA" class="a-tag">FPGA</a><span>&nbsp;</span></span></p><p class="post-abstract"><p>编译，广义上是将一种表达映射成另一种表达，维持功能不变。深度学习后端硬件的编译，是将DSL描述的计算图映射成后端硬件指令。</p>
<span id="more"></span>
<p>传统编译器直接解析文本，通过词法与语法分析，构建AST<a href="#refer-1"><sup>1</sup></a>。之后在AST上执行一系列的优化，称为优化pipeline。</p>
<p>优化后的AST可以转换为IR（比如LLVM<a href="#refer-2"><sup>2</sup></a> IR），由LLVM进一步优化，并生成后端代码。如果是添加新后端并且复用LLVM，需要实现对应Codegen逻辑。AST也可以不借助LLVM，直接编写codegen逻辑生成不同层级的code或指令。</p>
<p>随着深度学习模型越来越复杂，在生产中广泛使用，对计算性能和成本的需求日趋强烈，深度学习编译器在这样的背景下逐步发展壮大。</p>
<p>在深度学习编译器提出前，广泛使用的优化方法是手工fuse和手工codegen。但这样的劣势显而易见，模型中可能存在的子图pattern千奇百怪，并且各种自定义长尾算子层出不求，如果一直依靠手工优化，开发时间和人力成本都不太能令人接受。</p>
<p>因此，借鉴传统编译器的思路，通过有限的基础表达符号表示任意计算图结构，由编译器的codegen层统一做后端代码生成。</p>
<h1><a href="#fen-lei" class="header-anchor">#</a><span id="fen-lei"> 分类</span></h1>
<p>编译器根据编译发生时间的不同可以分为AOT<a href="#refer-3"><sup>3</sup></a>（Ahead Of Time）和JIT<a href="#refer-4"><sup>4</sup></a>（Just In Time）。</p>
<p>AOT编译通常用于standalone编译程序，由用户提前运行编译生成指令，并传输到运行设备上（非标设备/专用设备，比如ARM和定制NPU），通过runtime执行。</p>
<p>一次编译，到处运行，这允许AOT编译消耗更多的时间，探索更优结果，但也限制了灵活性。每当模型发生改动就需要触发一次编译行为，通常编译设备和运行设备是分立的，指令文件和runtime版本的同步会更复杂。</p>
<p>JIT在运行时条件触发编译，runtime和compiler捆绑，不存在版本不匹配的问题。JIT通常见于解释型程序，因为解释效率通常低于编译，通过运行前编译提高执行效率。</p>
<p>由于在运行时编译，编译时间相对运行时间必须要合理，这意味着不能引入过于复杂的优化逻辑，另外为了避免重复编译，可以加入cache（程序内/远程）提高效率。</p>
<h1><a href="#qian-duan" class="header-anchor">#</a><span id="qian-duan"> 前端</span></h1>
<p>编译器前端需要读取各个框架的模型文件，比如Tensorflow的SavedModel或者PyTorch的TorchScript。</p>
<p>也有试图将模型表达统一的工作出现，比如ONNX，试图在Caffe、PyTorch、MXNet等框架间构建转换桥梁。愿景很好，方便用户在不同框架迁移，不被框架锁死，但最终没有被大规模使用。</p>
<p>首先，Tensorflow出自Google，并不买账，ONNX是其他大公司的联盟产物，然而框架上只有PyTorch不断发展，和Tensorflow二分天下。</p>
<p>统一模型表达，还是在统一社区，也就是统一算子集。</p>
<p>Tensorflow曾凭借其绝对的垄断地位十分强势，后端软件栈都主动接入，接受Tensorflow的核心算子集为golden标准。然而不同框架的算子集相差很多，基本上只有最主流的算子可以无缝转换，换个复杂的模型很有可能就挂了。尤其是Tensorflow的计算图，控制结构十分复杂，连官方的优化工具都可能出现bug。如果模型定制程度高，不如直接改脚本来的快。</p>
<p>当然对大多数算法工程师来说，没有被框架锁定这件事，只有看哪个框架的生态的轮子能符合业务需求。</p>
<h1><a href="#you-hua" class="header-anchor">#</a><span id="you-hua"> 优化</span></h1>
<p>编译优化方法泛指一大类模型优化方法，比如Tensorflow自带的后端XLA，基于profiling搜索的TVM，基于整数线性规划的多面体方法等。</p>
<p>Tensorflow自带的前端计算图优化Grappler也可以算作编译器的优化功能。在图层面执行一般优化方法，比如CSE（公共子表达式消除），Constant Folding（常量折叠），去除无用节点（Dead Nodes）等。</p>
<p>对于硬件后端编译器，主要优化步骤如下：</p>
<ol start="0">
<li>硬件无关优化。</li>
<li>尽可能从子图中匹配更多符合硬件结构的pattern。</li>
<li>匹配可以转换为符合硬件结构pattern的子图，并实现转换逻辑（硬件相关优化），注意功能正确性。</li>
<li>尽量将匹配到的子图融合，使之最大化连通。</li>
<li>设定不连通子图最小阈值，防止主存和硬件内存的context switch开销过高，可以通过算子数量或者算力评估确定阈值，标记所有符合阈值的子图算子。</li>
<li>融合所有标记的子图，生成对应的模型IR或者硬件IR。</li>
<li>可以通过自定义Op的形式，将生成的IR序列化（作为Op参数），使用统一的Op（调用runtime）。</li>
<li>如果模型可以全量支持，则可以直接导出IR，使用runtime运行。</li>
</ol>
<p>进一步的，编译器可以独立为TensorRT的形式，允许用户使用API直接构建运行，并且可以通过JIT codegen dispatch手工优化的code。</p>
<p>编译器相比于计算库更关注图层面优化，底层会协调计算库的不同实现使全局性能最优。</p>
<hr>
<ul>
<li>[1] <span id="refer-1"></span> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract syntax tree</a></li>
<li>[2] <span id="refer-2"></span> <a target="_blank" rel="noopener" href="https://llvm.org/">The LLVM Compiler Infrastructure</a></li>
<li>[3] <span id="refer-3"></span> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ahead-of-time_compilation">Ahead-of-time compilation</a></li>
<li>[4] <span id="refer-4"></span> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Just-in-time_compilation">Just-in-time compilation</a></li>
</ul>
</p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a target="_blank" rel="noopener" href="http://twitter.com/home?status=https://blog.kleon.space/ai/ml-system/hardware-backend/9-compiler/%20KLEON%20机器学习系统 1-9 - 后端硬件编译器" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/ai/ml-system/hardware-backend/10-framework-integration/" title="机器学习系统 1-10 - 后端硬件框架接入"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: 机器学习系统 1-10 - 后端硬件框架接入</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/ai/ml-system/hardware-backend/8-runtime/" title="机器学习系统 1-8 - 后端硬件运行时">Next post: 机器学习系统 1-8 - 后端硬件运行时&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2022&nbsp;<a target="_blank" href="https://blog.kleon.space" rel="noopener noreferrer">Kleon</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>